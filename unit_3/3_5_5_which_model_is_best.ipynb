{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinkful Unit 3, Lesson 5, Section 5 - Which model is best for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinkful: You now have a fairly substantial starting toolbox of supervised learning methods that you can use to tackle a host of exciting problems. To make sure all of these ideas are organized in your mind, please go through the list of problems below. For each, identify which supervised learning method(s) would be best for addressing that particular problem. Explain your reasoning and discuss your answers with your mentor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Algorithm Selection from The Hundred Page Machine Learning Book by Andriiy Burkov (paraphrased)\n",
    "\n",
    "Here are some questions to ask yourself before starting to work on the problem. Depending on the answers, you could 'shortlist' some models.\n",
    "\n",
    "1. Explainability. Most of the very accurate algorithms as 'black boxes'. If you need to explain the factors that contribute most to a model's predictive value to a decision maker, then black box models aren't well suited. kNN, linear regression, or decision tree algorithms make their predictions in an understandable way.\n",
    "\n",
    "2. In memory vs outside-of-memory. If your dataset cannot be fully loaded into RAM and if you don't have access to a multi-GPU environment, then you want to choose an incremental learning algorithm. Incremental algorithms can take in new data as it comes, rather than having to re-train the model on the entire dataset. There are incremental versions of SVM and neural networks.\n",
    "\n",
    "\n",
    "3. Number of features, number of observations. Neural networks and gradient boosting can handle millions of features and a very large number of examples/observations.\n",
    "\n",
    "4. Categorical vs numerical features. You may need to convert categorical features to numeric features for some models.\n",
    "\n",
    "\n",
    "5. Non-linearity in the data. If data is not linearly separable or cannot be transformed to meet the required assumptions of a linear model, then you must try ensemble algorithms or a neural network.\n",
    "\n",
    "\n",
    "6. Training speed. How much time do you have to build the model? Do you have access to multiple CPU cores? Neural networks are slow to train. You may need to try simple algorithms first.\n",
    "\n",
    "\n",
    "7. Prediction speed. Consider how fast the model has to be when generating predictions. Algorithms that are fast at prediction time (have high throughput) include SVMs, linear and logistic regression and some kinds of neural networks are fast at prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict the running times of prospective Olympic sprinters using data from the last 20 Olympics.\n",
    "Best supervised learning method: KNN -- K Nearest Neighbor regression.\n",
    "\n",
    "Reasoning: This problem involves predicting a continuous variable. From this statement, I cannot tell how many meaningful features are contained in the data, so let's try predicting based on the proximity of those features to known values. Maybe the most important variable is the year the Olympics were held, since times have decreased steadily in the last 20 years. Times decreased more rapidly earlier in the 20-year period than later in that period. Other important variables could include the sprinter's height, weight, age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You have more features (columns) than rows in your dataset\n",
    "Best supervised learning method: Random forest.\n",
    "\n",
    "Reasoning: The number of obsevations is probably low. The \"Curse of dimensionality\" is in play here; you need more data points for each feature added for an accurate, valid model. Random forest \"bootstraps\" the data, which is appropriate when N is small. Random forest takes into account the variance in the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify the most important characteristic predicting likelihood of being jailed before age 20\n",
    "\n",
    "Best supervised learning method: Logistic regression\n",
    "\n",
    "Reasoning: This question requires being able to identify the parameter(s) that are most important. The outcome is binary: jailed or not jailed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement a filter to “highlight” emails that might be important to the recipient\n",
    "Best supervised learning method: Naive Bayes classifier\n",
    "\n",
    "Reasoning: The outcome is categorical. This problem is similar to one of the classic uses of Naive Bayes, which is as a spam filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You have 1000+ features\n",
    "Best supervised learning method: Use lasso as a method for regularization during fitting a logistic or linear regression model to reduce the number of features. \n",
    "\n",
    "Reasoning: This problem has a very large feature space. Lasso penalizes large coefficients and zeroes out the coefficients for variables that are not important. Those variables are dropped in the modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict whether someone who adds items to their cart on a website will purchase the items\n",
    "\n",
    "Best supervised learning method: Try random forest first.\n",
    "\n",
    "Reasoning: This is a classification problem: buy or not buy is the target variable. This is probably not suitable to a parametric ML algorithm (such as logistic regression), because we aren't sure of the form of the predictive function. In other words, we don't have much prior knowledge about the data and aren't able to choose the right features initially. kNN, decision trees and SVM are examples of non-parametric models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your dataset dimensions are 982400 x 500\n",
    "\n",
    "Best supervised learning method:  Regularization method, then a regression algorithm.\n",
    "\n",
    "Reasoning: (Assuming the first number is rows, we still have many rows, many features as in previous examples.) We can use a regularization method to reduce the features to manageable number, then use a regression algorithm that can perform well with large data. We need to be sure the data has the appropriate relationships and satisfies the assumptions for regression before using it, however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify faces in an image\n",
    "\n",
    "Best supervised learning method: kNN\n",
    "\n",
    "Reasoning: We can use kNN to find out which labeled images have the attributes most like the image we are trying to identify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict which of three flavors of ice cream will be most popular with boys vs girls.\n",
    "\n",
    "Best supervised learning method: Random forest\n",
    "\n",
    "Reasoning: Assuming there are multiple features that relate to the 3 ice cream flavors (i.e., flavor isn't the only feature), this is a classification problem (target being boy or girl) that we could use random forest on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SUPERVISED MODEL TYPES -- General "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifiers either assign a category to an outcome or assign a probability to each category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any classification learning algorithm that builds a model either implicitly or explicitly creates a decision boundary. A decision boundary can be straight, curved, or have a more complex form. The way the decision boundary is algorithmically or mathematically computed based on the training data -- its form -- differentiates one learning algorithm from another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression models have continuous outcome variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression is a problem of predicting a real-valued label (i.e., real number value, or the target) given an unlabeled example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SUPERVISED MODEL TYPES -- Detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLS Linear Regression\n",
    "\n",
    "Why use the linear form? 1) It's simple; one can avoid unnecessary complexity. 2) Linear models rarely overfit.\n",
    "\n",
    "Data that is to be fed into a linear model has to meet 3 assumption, however. Or, the data has to be transformed to meet those 3 criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression\n",
    "\n",
    "Logistic regression is actually a classification learning algorithm, not a regression. The name came about because of similarities in the mathematical foundations of the two models.\n",
    "\n",
    "Logistic regression models y as a function of x, but, y is binary. There are also methods for multi-class \n",
    "\n",
    "Logistic regression maximizes the likelihood of our training data according to the model. That is the optimization criteria that is used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "\n",
    "A type of classifier. Example: Filtering spam email messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kNN (k Nearest Neighbor)\n",
    "\n",
    "KNN can be used as a classifier model; or, as a regression model. KNN uses the distance between the new observation and K known observations to make a prediction of either a category or a value. \n",
    "\n",
    "kNN is a non-parametric algorithm. kNN keeps all training examples in memory so that it can find k training examples that are closest to the new 'x' example that comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree\n",
    "\n",
    "'ID3' is one common learning algorithm using a decision tree. Another is called C4.5\n",
    "\n",
    "Decision trees models attempt to reduce and eventually minimize entropy. Entropy is a measure of uncertainty about a random variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "Random forest can be used for classification or regression. Each decision tree in the forest gets a vote on the outcome of the new observation. Features are chosen at random for each iteration of the tree. Random forest is a type of ensemble model -- that is, the sub-models for a random forest are individual decision trees. Random forest uses bagging.¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble models\n",
    "\n",
    "Ensemble models are made up of other models. \n",
    "\n",
    "Ensemble models include bagging (a short form for \"bootstrap aggregregation\") and boosting.\n",
    "\n",
    "Bagging involves training a model on each of several subsets of the data. The models on subsets vote on the outcome simultaneously, taking the majority vote or mean value.\n",
    "\n",
    "Boosting is serial, not simultaneous. Boosting uses the output of one model as the input for the next.\n",
    "\n",
    "Ensemble models general perform well; they have low variance because they are built from multiple underlying models.\n",
    "Downsides are loss of transparency and being prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent\n",
    "\n",
    "This algoritm iteratively minimizes the cost function using derivatives. It is used for complex problems where we cannot find a true minimum by solving a system of equations (as in linear regression, by minimizing the distance between each data point and the line of fit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine\n",
    "\n",
    "Support vector machine works as a binary classifier, a multi-class classifier, and as a regressor.\n",
    "\n",
    "A simple binary classifier is easiest to understand. The ideal is to find an optimal boundary line with the largest margins between negative and positive examples in the training data.\n",
    "\n",
    "Multi-class classifiers can use holdouts and run a binary classifier many times. Or, they can be done pairwise, where an observation is categorized under every possible pair of outcomes, and then the outcome is assigned to the one that was most common.\n",
    "\n",
    "SVM can also be used as an unsupervised clustering algorithm.\n",
    "\n",
    "SVM is \"...reasonably accurate modeling option for both regression and classification. The ability to define a margin also allows for creating acceptable error for training in the regression context, a useful feature when you decide that's how you'd like to model your data.\" [source: Thinkful curriculum]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression\n",
    "\n",
    "Ridge and lasso use a different cost function than linear regression. They try to optimize variance in the test set, whereas OLS tries to optimize variance in the training set.\n",
    "\n",
    "Ridge regression imposes a penalty for large coefficients. It works to reduce overfitting as compared to OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Regression\n",
    "\n",
    "Lasso tries to force small parameter coefficients to zero, so they can be dropped. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - \"Ridge regression is an excellent tool to use with correlated features, while lasso is an efficient method of feature selection when dealing with an unmanageably large feature space.\" [source: Thinkful curriculum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
