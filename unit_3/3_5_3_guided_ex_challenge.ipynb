{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boost guided example\n",
    "\n",
    "Having walked through gradient boost by hand, now let's try it with SKlearn.  We'll still use the European Social Survey Data, but now with a categorical outcome: Whether or not someone lives with a partner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv((\n",
    "    \"https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/\"\n",
    "    \"master/ESS_practice_data/ESSdata_Thinkful.csv\")).dropna()\n",
    "\n",
    "\n",
    "# Definine outcome and predictors.\n",
    "# Set our outcome to 0 and 1.\n",
    "y = df['partner'] - 1\n",
    "X = df.loc[:, ~df.columns.isin(['partner', 'cntry', 'idno'])]\n",
    "\n",
    "# Make the categorical variable 'country' into dummies.\n",
    "X = pd.concat([X, pd.get_dummies(df['cntry'])], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# # Create training and test sets.\n",
    "# offset = int(X.shape[0] * 0.9)\n",
    "\n",
    "# # Put 90% of the data in the training set.\n",
    "# X_train, y_train = X[:offset], y[:offset]\n",
    "\n",
    "# # And put 10% in the test set.\n",
    "# X_test, y_test = X[offset:], y[offset:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're now working with a binary outcome, we've switched to a classifier.  Now our loss function can't be the residuals.  Our options are \"deviance\", or \"exponential\".  Deviance is used for logistic regression, and we'll try that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run description: baseline\n",
      "\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.04603345097437471\n",
      "Percent Type II errors: 0.1752340033757864\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.05214723926380368\n",
      "Percent Type II errors: 0.20368098159509201\n"
     ]
    }
   ],
   "source": [
    "# We'll make 500 iterations, use 2-deep trees, and set our loss function.\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'deviance'}\n",
    "run_desc = 'baseline'\n",
    "\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Run description: {}\\n\\n'\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(run_desc, train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n",
    "\n",
    "results_df = pd.DataFrame(columns = ['params', 'train_type1', 'train_type2', 'test_type1', 'test_type2'])\n",
    "new_results = {'params': params, 'run_desc' : run_desc, 'train_type1': train_tI_errors, 'train_type2': train_tII_errors, 'test_type1': test_tI_errors, 'test_type2': test_tII_errors}\n",
    "results_df = results_df.append(new_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike decision trees, gradient boost solutions are not terribly easy to interpret on the surface.  But they aren't quite a black box.  We can get a measure of how important various features are by counting how many times a feature is used over the course of many decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109ec2240>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importance = clf.feature_importances_\n",
    "\n",
    "# Make importances relative to max importance.\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, X.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that age and happiness are the most important features in predicting whether or not someone lives with a partner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### DRILL: Improve this gradient boost model\n",
    "\n",
    "While this model is already doing alright, we've seen from the Type I and Type II error rates that there is definitely room for improvement.  Your task is to see how low you can get the error rates to go in the test set, based on your model in the training set.  Strategies you might use include:\n",
    "\n",
    "* Creating new features\n",
    "* Applying more overfitting-prevention strategies like subsampling\n",
    "* More iterations\n",
    "* Trying a different loss function\n",
    "* Changing the structure of the weak learner: Allowing more leaves in the tree, or other modifications\n",
    "\n",
    "Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robin's work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Try a different loss function\": There are only two choices among the parameters for GradientBoostingClassifier. The choices are the default, 'deviance', and 'exponential'. The documentation says, \" For loss ‘exponential’ gradient boosting recovers the AdaBoost algorithm.\". I don't understand what the reference to 'AdaBoost' means.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run description: chg loss function\n",
      "\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.0469541199938622\n",
      "Percent Type II errors: 0.17600122755869266\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.05214723926380368\n",
      "Percent Type II errors: 0.19938650306748465\n"
     ]
    }
   ],
   "source": [
    " # We'll make 500 iterations, use 2-deep trees, and set our loss function.\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'exponential'}\n",
    "run_desc = 'chg loss function'\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Run description: {}\\n\\n'\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(run_desc, train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n",
    "\n",
    "new_results = {'params': params, 'run_desc': run_desc, 'train_type1': train_tI_errors, 'train_type2': train_tII_errors, 'test_type1': test_tI_errors, 'test_type2': test_tII_errors}\n",
    "results_df = results_df.append(new_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the loss function results in slightly higher error rates in the test set as compared to the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try increasing the depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run description: incr depth\n",
      "\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.020101273592143625\n",
      "Percent Type II errors: 0.1126285100506368\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.06993865030674846\n",
      "Percent Type II errors: 0.19386503067484662\n"
     ]
    }
   ],
   "source": [
    " # We'll make 500 iterations, use 2-deep trees, and set our loss function.\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 4,\n",
    "          'loss': 'deviance'}\n",
    "run_desc = 'incr depth'\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Run description: {}\\n\\n'\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(run_desc, train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n",
    "\n",
    "new_results = {'params': params, 'run_desc': run_desc, 'train_type1': train_tI_errors, 'train_type2': train_tII_errors, 'test_type1': test_tI_errors, 'test_type2': test_tII_errors}\n",
    "results_df = results_df.append(new_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When increasing the depth, Type I errors in the test set go up by about 2/10 of a percent but Type II errors go down by ~ half a percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \"Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\"\n",
    " Try increasing the number of estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run description: incr num estimators\n",
      "\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.04541967162804972\n",
      "Percent Type II errors: 0.1704772134417677\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.05460122699386503\n",
      "Percent Type II errors: 0.20122699386503068\n"
     ]
    }
   ],
   "source": [
    "# We'll make 500 iterations, use 2-deep trees, and set our loss function.\n",
    "params = {'n_estimators': 750,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'deviance'}\n",
    "run_desc = 'incr num estimators'\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Run description: {}\\n\\n'\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(run_desc, train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n",
    "\n",
    "new_results = {'params': params, 'run_desc': run_desc, 'train_type1': train_tI_errors, 'train_type2': train_tII_errors, 'test_type1': test_tI_errors, 'test_type2': test_tII_errors}\n",
    "results_df = results_df.append(new_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increasing estimators by one-half: ~ 1/2 percent increase in Type I errors, ~ 1/2 percent decrease in Type II errors in test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try sub-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run description: use subsample 0.8\n",
      "\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.04664723032069971\n",
      "Percent Type II errors: 0.17093754795151142\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.05521472392638037\n",
      "Percent Type II errors: 0.19754601226993865\n"
     ]
    }
   ],
   "source": [
    "# We'll make 500 iterations, use 2-deep trees, and set our loss function.\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'deviance',\n",
    "         'subsample': 0.8}\n",
    "run_desc = 'use subsample 0.8'\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Run description: {}\\n\\n'\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(run_desc, train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n",
    "\n",
    "new_results = {'params': params, 'run_desc': run_desc, 'train_type1': train_tI_errors, 'train_type2': train_tII_errors, 'test_type1': test_tI_errors, 'test_type2': test_tII_errors}\n",
    "results_df = results_df.append(new_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run description: use subsample 0.90\n",
      "\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.0475678993401872\n",
      "Percent Type II errors: 0.1698634340954427\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.05337423312883435\n",
      "Percent Type II errors: 0.19570552147239265\n"
     ]
    }
   ],
   "source": [
    "# We'll make 500 iterations, use 2-deep trees, and set our loss function.\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'deviance',\n",
    "         'subsample': 0.90}\n",
    "run_desc = 'use subsample 0.90'\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Run description: {}\\n\\n'\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(run_desc, train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n",
    "\n",
    "new_results = {'params': params, 'run_desc': run_desc, 'train_type1': train_tI_errors, 'train_type2': train_tII_errors, 'test_type1': test_tI_errors, 'test_type2': test_tII_errors}\n",
    "results_df = results_df.append(new_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run description: incr depth and subsample 0.70\n",
      "\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.02071505293846862\n",
      "Percent Type II errors: 0.09958569894123062\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.06809815950920245\n",
      "Percent Type II errors: 0.18773006134969325\n"
     ]
    }
   ],
   "source": [
    "# We'll make 500 iterations, use 2-deep trees, and set our loss function.\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 4,\n",
    "          'loss': 'deviance',\n",
    "         'subsample': 0.70}\n",
    "run_desc = 'incr depth and subsample 0.70'\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Run description: {}\\n\\n'\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(run_desc, train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n",
    "\n",
    "new_results = {'params': params, 'run_desc': run_desc, 'train_type1': train_tI_errors, 'train_type2': train_tII_errors, 'test_type1': test_tI_errors, 'test_type2': test_tII_errors}\n",
    "results_df = results_df.append(new_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              params  train_type1  \\\n",
      "0  {'n_estimators': 500, 'max_depth': 2, 'loss': ...     0.046033   \n",
      "1  {'n_estimators': 500, 'max_depth': 2, 'loss': ...     0.046954   \n",
      "2  {'n_estimators': 500, 'max_depth': 4, 'loss': ...     0.020101   \n",
      "3  {'n_estimators': 750, 'max_depth': 2, 'loss': ...     0.045420   \n",
      "4  {'n_estimators': 500, 'max_depth': 2, 'loss': ...     0.046647   \n",
      "5  {'n_estimators': 500, 'max_depth': 2, 'loss': ...     0.048182   \n",
      "7  {'n_estimators': 500, 'max_depth': 2, 'loss': ...     0.047568   \n",
      "8  {'n_estimators': 500, 'max_depth': 4, 'loss': ...     0.020715   \n",
      "\n",
      "   train_type2  test_type1  test_type2                       run_desc  \n",
      "0     0.175234    0.052147    0.203681                       baseline  \n",
      "1     0.176001    0.052147    0.199387              chg loss function  \n",
      "2     0.112629    0.069939    0.193865                     incr depth  \n",
      "3     0.170477    0.054601    0.201227            incr num estimators  \n",
      "4     0.170938    0.055215    0.197546              use subsample 0.8  \n",
      "5     0.170324    0.052147    0.195092             use subsample 0.70  \n",
      "7     0.169863    0.053374    0.195706             use subsample 0.90  \n",
      "8     0.099586    0.068098    0.187730  incr depth and subsample 0.70  \n"
     ]
    }
   ],
   "source": [
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After experimenting with various parameters, I found that the following parameters produced the best results:\n",
    "### Depth = 4 (2x the baseline) and subsample fraction of 0.70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "59px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
